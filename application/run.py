import boto3
import subprocess  # nosec B404
import os
import sys
from utils.evalution_criteria import calculate_sharpe_ratio,check_required_columns
from utils.runtime_checks import run_script
from utils.static_checks import perform_static_checks
from utils.replace_func import update_script_with_template_functions
import pandas as pd
from utils.boto3_helper import store_sharpe_ratio_in_dynamodb,send_failure_email,upload_script_to_s3

USER_SCRIPTS_BUCKET_NAME = os.environ.get("USER_SCRIPTS_BUCKET_NAME", "jkpfactors-user-scripts")
INTEGRITY_CHECK_DATA_S3_URI = "s3://jkpfactors-training-data/integrity-check/"
INTEGRITY_CHECK_DATA_LOCAL_PATH = "integrity-check/"
COMPLETE_DATA_S3_URI = "s3://jkpfactors-training-data/complete/"
COMPLETE_DATA_PATH = "data/"
SHOULD_PERFORM_COMPLETE_TRAINING= os.environ.get("SHOULD_PERFORM_COMPLETE_TRAINING",True)

def download_from_s3(s3_uri, local_path):
    s3 = boto3.client('s3')
    bucket, key = s3_uri.replace("s3://", "").split("/", 1)
    
    if not key.endswith('/'):
        s3.download_file(bucket, key, local_path)
    else:
        paginator = s3.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket=bucket, Prefix=key)
        
        for page in pages:
            for obj in page.get('Contents', []):
                file_key = obj['Key']
                if file_key.endswith('/'):
                    # Skip folders within the folder
                    continue
                # Create local directories if they do not exist
                local_file_path = os.path.join(local_path, os.path.relpath(file_key, key))
                os.makedirs(os.path.dirname(local_file_path), exist_ok=True)
                s3.download_file(bucket, file_key, local_file_path)

def perform_integrity_check(script_file, output_file):
    
    # Step 1: Static Checks
    is_valid, message = perform_static_checks(script_file)
    if not is_valid:
        return False, message
    
    # Step 2: Check the output csv if it contains all required columns.
    is_valid, message = check_required_columns(output_file)
    if not is_valid:
        return False, message

    # Step 3: Runtime Checks
    template_functions_mapping = {
        'load_data': 'templates/integrity_check/load_func.py',
        'export_data': 'templates/integrity_check/export_func.py',
    }
    update_script_with_template_functions(script_file,template_functions_mapping)
    
    # Step 4: replace data loading and export function in the script file
    is_valid, message = run_script(script_file)
    if not is_valid:
        return False, message
    
    # Step 4: Check the output csv generated by script if it contains all required columns.
    is_valid, message = check_required_columns("integrity-check/training_results.csv")
    if not is_valid:
        return False, message

    return True, "All checks passed"

def main(user_ml_script_s3_uri, user_ml_output_csv_s3_uri,submission_timestamp,email,user_name,model_name):
    # Define local paths
    script_file = user_ml_script_s3_uri.split("/")[-1]
    output_file = user_ml_output_csv_s3_uri.split("/")[-1]
    
    # Download the files from S3
    download_from_s3(user_ml_script_s3_uri, script_file)
    download_from_s3(user_ml_output_csv_s3_uri, output_file)
    download_from_s3(INTEGRITY_CHECK_DATA_S3_URI,INTEGRITY_CHECK_DATA_LOCAL_PATH)


    
    # Run integrity checks
    is_valid, message = perform_integrity_check(script_file, output_file)
    print(message)
    
    if is_valid:
        upload_script_to_s3(script_file,output_file,USER_SCRIPTS_BUCKET_NAME,email,submission_timestamp)
    else:
        send_failure_email(email=email,message=message)
        
        
    if is_valid and SHOULD_PERFORM_COMPLETE_TRAINING:
        
        # Step 1: Replace data loading and export function in the script file
        download_from_s3(COMPLETE_DATA_S3_URI,COMPLETE_DATA_PATH)
        
        # Step 2: Replace data loading and export function in the script file
        template_functions_mapping = {
            'load_data': 'templates/complete_data/load_func.py',
            'export_data': 'templates/complete_data/export_func.py',
        }
        update_script_with_template_functions(script_file,template_functions_mapping)
        
        # Step 3: Run the downloaded script
        is_valid, message = run_script(script_file)
        if not is_valid:
            return False, message
        
        # Step 4: Check output columns
        is_valid, message = check_required_columns("data/training_results.csv")
        if not is_valid:
            return False, message
        
        # Step 5: Evaluation Criteria
        is_valid, message = calculate_sharpe_ratio()
        if not is_valid:
            return False, message
        
        store_sharpe_ratio_in_dynamodb(sharpe_ratio=message,submission_timestamp=submission_timestamp,email=email,user_name=user_name,model_name=model_name)
        
    else:
        print("Integrity checks failed. Exiting.")

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='Run integrity checks and execute user ML script.')
    parser.add_argument('--user_ml_script_s3_uri', type=str, help='The S3 URI of the user ML script.')
    parser.add_argument('--user_ml_output_csv_s3_uri', type=str, help='The S3 URI of the user ML output CSV.')
    parser.add_argument('--submission_timestamp', type=str, help='The S3 URI of the user ML output CSV.')
    parser.add_argument('--email', type=str, help='The S3 URI of the user ML output CSV.')
    parser.add_argument('--user_name', type=str, help='The S3 URI of the user ML output CSV.')
    parser.add_argument('--model_name', type=str, help='The S3 URI of the user ML output CSV.')

    args = parser.parse_args()
    main(args.user_ml_script_s3_uri, args.user_ml_output_csv_s3_uri,args.submission_timestamp,args.email,args.user_name,args.model_name)